{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mohse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (4.13.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mohse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\mohse\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from beautifulsoup4) (4.13.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\mohse\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys , os\n",
    "!{sys.executable} -m pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] =sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\mohse\\AppData\\Local\\Temp\\ipykernel_17344\\4148745384.py:1: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  os.environ[\"HADOOP_HOME\"] = \"C:\\SPARK\\spark-3.5.4-bin-hadoop3\\spark-3.5.4-bin-hadoop3\"\n",
      "C:\\Users\\mohse\\AppData\\Local\\Temp\\ipykernel_17344\\4148745384.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  os.environ[\"HADOOP_OPTS\"] = \"-Djava.library.path=C:\\SPARK\\spark-3.5.4-bin-hadoop3\\spark-3.5.4-bin-hadoop3\\bin\"\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HADOOP_HOME\"] = \"C:\\SPARK\\spark-3.5.4-bin-hadoop3\\spark-3.5.4-bin-hadoop3\"  \n",
    "os.environ[\"HADOOP_OPTS\"] = \"-Djava.library.path=C:\\SPARK\\spark-3.5.4-bin-hadoop3\\spark-3.5.4-bin-hadoop3\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"XML Reader\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.17.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"posts_Spark_Cleansing\")\\\n",
    "    .config(\"spark.executor.memory\", \"3g\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.16.0,org.apache.parquet:parquet-hadoop:1.15.1\")\\\n",
    "    .config(\"spark.pyspark.python\", sys.executable) \\\n",
    "    .config(\"spark.pyspark.driver.python\", sys.executable) \\\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"false\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+---+-------+---------+-------+\n",
      "|_Class|               _Date|_Id|  _Name|_TagBased|_UserId|\n",
      "+------+--------------------+---+-------+---------+-------+\n",
      "|     3|2010-07-19 22:39:...|  1|Teacher|    false|      5|\n",
      "|     3|2010-07-19 22:39:...|  2|Teacher|    false|      6|\n",
      "|     3|2010-07-19 22:39:...|  3|Teacher|    false|      8|\n",
      "|     3|2010-07-19 22:39:...|  4|Teacher|    false|     23|\n",
      "|     3|2010-07-19 22:39:...|  5|Teacher|    false|     36|\n",
      "|     3|2010-07-19 22:39:...|  6|Teacher|    false|     37|\n",
      "|     3|2010-07-19 22:39:...|  7|Teacher|    false|     50|\n",
      "|     3|2010-07-19 22:39:...|  8|Teacher|    false|     55|\n",
      "|     3|2010-07-19 22:39:...|  9|Student|    false|      5|\n",
      "|     3|2010-07-19 22:39:...| 10|Student|    false|      8|\n",
      "|     3| 2010-07-19 22:39:08| 11|Student|    false|     13|\n",
      "|     3|2010-07-19 22:39:...| 12|Student|    false|     18|\n",
      "|     3|2010-07-19 22:39:...| 13|Student|    false|     23|\n",
      "|     3|2010-07-19 22:39:...| 14|Student|    false|     24|\n",
      "|     3|2010-07-19 22:39:...| 16|Student|    false|     59|\n",
      "|     3|2010-07-19 22:39:...| 17|Student|    false|     66|\n",
      "|     3|2010-07-19 22:39:...| 18|Student|    false|     69|\n",
      "|     3|2010-07-19 22:39:...| 19|Student|    false|     75|\n",
      "|     3|2010-07-19 22:39:...| 20| Editor|    false|     13|\n",
      "|     3|2010-07-19 22:39:...| 21| Editor|    false|     23|\n",
      "+------+--------------------+---+-------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "badges_df = spark.read.format(\"xml\") \\\n",
    "    .option(\"rowTag\", \"row\") \\\n",
    "    .load(\"badges.xml\")\n",
    "\n",
    "badges_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate nulls in each column\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "null_perc=badges_df.select([((count(when(col(c).isNull(),c)) / badges_df.count()) * 100).alias(c) for c in badges_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+-----+---------+-------+\n",
      "|_Class|_Date|_Id|_Name|_TagBased|_UserId|\n",
      "+------+-----+---+-----+---------+-------+\n",
      "|   0.0|  0.0|0.0|  0.0|      0.0|    0.0|\n",
      "+------+-----+---+-----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_perc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+\n",
      "|_Class|_Id|  _Name|\n",
      "+------+---+-------+\n",
      "|     3|  1|Teacher|\n",
      "|     3|  2|Teacher|\n",
      "|     3|  3|Teacher|\n",
      "|     3|  4|Teacher|\n",
      "|     3|  5|Teacher|\n",
      "|     3|  6|Teacher|\n",
      "|     3|  7|Teacher|\n",
      "|     3|  8|Teacher|\n",
      "|     3|  9|Student|\n",
      "|     3| 10|Student|\n",
      "|     3| 11|Student|\n",
      "|     3| 12|Student|\n",
      "|     3| 13|Student|\n",
      "|     3| 14|Student|\n",
      "|     3| 16|Student|\n",
      "|     3| 17|Student|\n",
      "|     3| 18|Student|\n",
      "|     3| 19|Student|\n",
      "|     3| 20| Editor|\n",
      "|     3| 21| Editor|\n",
      "+------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Badge_Disc_Dim=badges_df.drop(*[\"_TagBased\",\"_Date\",\"_UserId\"])\n",
    "Badge_Disc_Dim.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------+\n",
      "|Class|Badge_Disc_BK|   Name|\n",
      "+-----+-------------+-------+\n",
      "|    3|            1|Teacher|\n",
      "|    3|            2|Teacher|\n",
      "|    3|            3|Teacher|\n",
      "|    3|            4|Teacher|\n",
      "|    3|            5|Teacher|\n",
      "|    3|            6|Teacher|\n",
      "|    3|            7|Teacher|\n",
      "|    3|            8|Teacher|\n",
      "|    3|            9|Student|\n",
      "|    3|           10|Student|\n",
      "|    3|           11|Student|\n",
      "|    3|           12|Student|\n",
      "|    3|           13|Student|\n",
      "|    3|           14|Student|\n",
      "|    3|           16|Student|\n",
      "|    3|           17|Student|\n",
      "|    3|           18|Student|\n",
      "|    3|           19|Student|\n",
      "|    3|           20| Editor|\n",
      "|    3|           21| Editor|\n",
      "+-----+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming Columns Names\n",
    "for col_name in Badge_Disc_Dim.columns:\n",
    "    Badge_Disc_Dim = Badge_Disc_Dim.withColumnRenamed(col_name, col_name.lstrip(\"_\"))\n",
    "\n",
    "Badge_Disc_Dim=Badge_Disc_Dim.withColumnRenamed(\"Id\",\"Badge_Disc_BK\")\n",
    "Badge_Disc_Dim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+---------+-------+\n",
      "|_Class|     _Date|_Id|  _Name|_TagBased|_UserId|\n",
      "+------+----------+---+-------+---------+-------+\n",
      "|     3|2010-07-19|  1|Teacher|    false|      5|\n",
      "|     3|2010-07-19|  2|Teacher|    false|      6|\n",
      "|     3|2010-07-19|  3|Teacher|    false|      8|\n",
      "|     3|2010-07-19|  4|Teacher|    false|     23|\n",
      "|     3|2010-07-19|  5|Teacher|    false|     36|\n",
      "|     3|2010-07-19|  6|Teacher|    false|     37|\n",
      "|     3|2010-07-19|  7|Teacher|    false|     50|\n",
      "|     3|2010-07-19|  8|Teacher|    false|     55|\n",
      "|     3|2010-07-19|  9|Student|    false|      5|\n",
      "|     3|2010-07-19| 10|Student|    false|      8|\n",
      "|     3|2010-07-19| 11|Student|    false|     13|\n",
      "|     3|2010-07-19| 12|Student|    false|     18|\n",
      "|     3|2010-07-19| 13|Student|    false|     23|\n",
      "|     3|2010-07-19| 14|Student|    false|     24|\n",
      "|     3|2010-07-19| 16|Student|    false|     59|\n",
      "|     3|2010-07-19| 17|Student|    false|     66|\n",
      "|     3|2010-07-19| 18|Student|    false|     69|\n",
      "|     3|2010-07-19| 19|Student|    false|     75|\n",
      "|     3|2010-07-19| 20| Editor|    false|     13|\n",
      "|     3|2010-07-19| 21| Editor|    false|     23|\n",
      "+------+----------+---+-------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, to_timestamp ,to_date\n",
    "\n",
    "badges_df = badges_df.withColumn(\"_Date\", date_format(to_timestamp(\"_Date\", \"yyyy-MM-dd HH:mm:ss\"), \"yyyy-MM-dd\"))\n",
    "badges_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+-------+\n",
      "|      date|Badge_Disc_fk|TagBased|User_fk|\n",
      "+----------+-------------+--------+-------+\n",
      "|2010-07-19|            1|   false|      5|\n",
      "|2010-07-19|            2|   false|      6|\n",
      "|2010-07-19|            3|   false|      8|\n",
      "|2010-07-19|            4|   false|     23|\n",
      "|2010-07-19|            5|   false|     36|\n",
      "|2010-07-19|            6|   false|     37|\n",
      "|2010-07-19|            7|   false|     50|\n",
      "|2010-07-19|            8|   false|     55|\n",
      "|2010-07-19|            9|   false|      5|\n",
      "|2010-07-19|           10|   false|      8|\n",
      "|2010-07-19|           11|   false|     13|\n",
      "|2010-07-19|           12|   false|     18|\n",
      "|2010-07-19|           13|   false|     23|\n",
      "|2010-07-19|           14|   false|     24|\n",
      "|2010-07-19|           16|   false|     59|\n",
      "|2010-07-19|           17|   false|     66|\n",
      "|2010-07-19|           18|   false|     69|\n",
      "|2010-07-19|           19|   false|     75|\n",
      "|2010-07-19|           20|   false|     13|\n",
      "|2010-07-19|           21|   false|     23|\n",
      "+----------+-------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col ,to_date\n",
    "\n",
    "\n",
    "badges_df = badges_df.withColumnRenamed(\"_Date\", \"date\") \\\n",
    "                     .withColumnRenamed(\"_Id\", \"Badge_Disc_fk\") \\\n",
    "                     .withColumnRenamed(\"_UserId\", \"User_fk\") \\\n",
    "                     .withColumnRenamed(\"_TagBased\", \"TagBased\")\n",
    "\n",
    "badges_df = badges_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "badges_fact = badges_df.select(\"date\", \"Badge_Disc_fk\", \"TagBased\", \"User_fk\")\n",
    "\n",
    "badges_fact.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- Badge_Disc_fk: long (nullable = true)\n",
      " |-- TagBased: boolean (nullable = true)\n",
      " |-- User_fk: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "badges_fact.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Badge_Disc_Dim.coalesce(1).write.mode(\"overwrite\").parquet(\"SilverDataSet/Badge_Disc_Dim\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "badges_fact.coalesce(1).write.mode(\"overwrite\").parquet(\"SilverDataSet/badges_fact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
